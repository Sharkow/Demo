{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import sklearn.preprocessing as skp\n",
    "import sklearn.compose as skc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SETTINGS.json') as settings_file:  \n",
    "    settings = json.load(settings_file)\n",
    "comp_data = settings['COMPETITION_DATA_DIR']\n",
    "pre_trained_model = settings['PRE_TRAINED_MODEL_PATH']\n",
    "submissions = settings['SUBMISSION_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define functions with preprocessing routines. Not running anything yet.\n",
    "# Data cleaning, fixing, feature generation.\n",
    "# We do everything in functional style:\n",
    "# main function here is get_train_data() - it builds the final matrix right from the raw data.\n",
    "# We then use it for training and validation.\n",
    "\n",
    "# Manually found shop duplicates, inferred by their names.\n",
    "def fix_shop_duplicates(df) -> None:\n",
    "    # Якутск Орджоникидзе, 56\n",
    "    df.loc[df.shop_id == 0, 'shop_id'] = 57\n",
    "    # Якутск ТЦ \"Центральный\"\n",
    "    df.loc[df.shop_id == 1, 'shop_id'] = 58\n",
    "    # Жуковский ул. Чкалова 39м²\n",
    "    df.loc[df.shop_id == 10, 'shop_id'] = 11\n",
    "\n",
    "def fixed_transactions() -> pd.DataFrame:\n",
    "    transactions = pd.read_csv(comp_data + 'sales_train.csv.gz')\n",
    "    fix_shop_duplicates(transactions)\n",
    "    return transactions\n",
    "\n",
    "# Group transactions by shop, item and month\n",
    "def grouped_sales() -> pd.DataFrame:\n",
    "    transactions = fixed_transactions()\n",
    "    grouped = transactions.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False)\n",
    "    grouped = grouped.agg({'item_cnt_day': np.sum})\\\n",
    "                     .rename(columns={'item_cnt_day': 'item_cnt_month'})\n",
    "    return grouped\n",
    "\n",
    "# Here is the most important part - make monthly sales for each item-shop,\n",
    "# this is the main thing we need to train on.\n",
    "def make_monthly_sales() -> pd.DataFrame:\n",
    "    items = pd.read_csv(comp_data + 'items.csv')\n",
    "    shops = get_shops()\n",
    "    dates = pd.DataFrame()\n",
    "    dates['date_block_num'] = pd.Series(range(0,33+1))\n",
    "    \n",
    "    # foo=1 is a pandas-way to make Cartesian product (without for-loops)\n",
    "    # So we first make a matrix with with all possible shop-item-months...\n",
    "    shop_item_months = \\\n",
    "        dates.assign(foo=1)\\\n",
    "        .merge(shops['shop_id'].to_frame().assign(foo=1))\\\n",
    "        .merge(items['item_id'].to_frame().assign(foo=1))\\\n",
    "        .drop('foo', 1)\n",
    "    # ...and then merge actual sales to it. Missing values mean zero sales, hence, fillna(0)\n",
    "    return shop_item_months.merge(grouped_sales(), how = 'left', on = ['shop_id', 'item_id', 'date_block_num'])\\\n",
    "                           .fillna(0)\n",
    "\n",
    "####################################################\n",
    "# Here we add a \"city\" feature to the dataset, extracting it from shop names.\n",
    "# We append the label-encoded version to the resulting data.\n",
    "def add_city_feature(shops) -> None:\n",
    "    shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "    shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "    shops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "    shops['city_code'] = skp.LabelEncoder().fit_transform(shops['city'])\n",
    "    shops = shops[['shop_id','city_code']]\n",
    "\n",
    "def get_shops() -> pd.DataFrame:\n",
    "    result = pd.read_csv(comp_data + 'shops.csv').query('shop_id not in [0, 1, 10]')#filter duplicates shops here\n",
    "    add_city_feature(result)\n",
    "    return result\n",
    "    \n",
    "def append_city_code(df) -> pd.DataFrame:\n",
    "    return df.merge(get_shops()[['shop_id', 'city_code']], how='left', on = ['shop_id'])\n",
    "####################################################\n",
    "\n",
    "####################################################\n",
    "# lag_feature() allows to add a lag of an arbitrary feature and time period\n",
    "def lag_feature(df, lags, col) -> pd.DataFrame:\n",
    "    tmp = df[['date_block_num','shop_id','item_id',col]]\n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] += i\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df\n",
    "\n",
    "# But be use only the simplest case - sales from the previous month.\n",
    "# We also have to remove the first month from the data set, since we can't provide this feature for it.\n",
    "def append_last_month_sales(df) -> pd.DataFrame:\n",
    "    df = lag_feature(df, [1], 'item_cnt_month')\n",
    "    df.query('date_block_num != 0', inplace=True)\n",
    "    return df\n",
    "####################################################\n",
    "\n",
    "####################################################\n",
    "# Main functions here - build train and test matrices, that we can feed to the model.\n",
    "# Simple caching is used here, so that we can loosely call these functions later,\n",
    "# not worrying for performance. Only the first call will be expensive.\n",
    "\n",
    "# We will split this later into test and validation sets\n",
    "def get_train_data() -> pd.DataFrame:\n",
    "    if get_train_data.cache is None:\n",
    "        print('making train data...')\n",
    "        get_train_data.cache = make_monthly_sales()\n",
    "        \n",
    "        # My experiments showed, that training only on item-shops, that appear in the test set,\n",
    "        # actually _improves_ the validation score slightly!\n",
    "        # And obviously, it is way faster.\n",
    "        #\n",
    "        # Another point is - validating only on test item-shops gives a very much closer score to the LB score.\n",
    "        # That means, item-shops from the test set are much harder to predict\n",
    "        # than average item-shop from the train set.\n",
    "        #\n",
    "        # So, we leave only item-shops from the test set.\n",
    "        test_set = pd.read_csv(comp_data + 'test.csv.gz')\n",
    "        fix_shop_duplicates(test_set)\n",
    "        get_train_data.cache =\\\n",
    "            get_train_data.cache.query('item_id in(@test_set.item_id) and shop_id in(@test_set.shop_id)')\n",
    "        \n",
    "        # adding city code feature\n",
    "        get_train_data.cache = append_city_code(get_train_data.cache)\n",
    "        # adding month feature (from 1 to 12)\n",
    "        get_train_data.cache['month_of_year'] = get_train_data.cache.date_block_num % 12 + 1\n",
    "        # adding item category feature\n",
    "        items = pd.read_csv(comp_data + 'items.csv')\n",
    "        get_train_data.cache = get_train_data.cache.merge(items[['item_id', 'item_category_id']], how='left', on='item_id')\n",
    "        # adding last month sales feature\n",
    "        get_train_data.cache = append_last_month_sales(get_train_data.cache)\n",
    "        # We will use clipped sales for training and validation\n",
    "        get_train_data.cache.item_cnt_month = get_train_data.cache.item_cnt_month.clip(0, 20)\n",
    "    return get_train_data.cache.copy()\n",
    "get_train_data.cache = None\n",
    "\n",
    "def get_test_data() -> pd.DataFrame:\n",
    "    if get_test_data.cache is None:\n",
    "        print('making test data...')\n",
    "        get_test_data.cache = pd.read_csv(comp_data + 'test.csv.gz')\n",
    "        fix_shop_duplicates(get_test_data.cache)\n",
    "        get_test_data.cache = append_city_code(get_test_data.cache)\n",
    "        \n",
    "        items = pd.read_csv(comp_data + 'items.csv')[['item_id', 'item_category_id']]\n",
    "        get_test_data.cache = get_test_data.cache.merge(items, how='left', on = ['item_id'])\n",
    "        get_test_data.cache['date_block_num'] = np.full((get_test_data.cache.shape[0]), 34)\n",
    "        get_test_data.cache['month_of_year'] = np.full((get_test_data.cache.shape[0]), 11)\n",
    "    \n",
    "        # append last month sales to test, using train data\n",
    "        sales_train = get_train_data().query('date_block_num == 33')\\\n",
    "                                      [['shop_id', 'item_id', 'item_cnt_month']]\\\n",
    "                                      .rename(columns={'item_cnt_month': 'item_cnt_month_lag_1'})\n",
    "        get_test_data.cache = get_test_data.cache.merge(sales_train, on=['shop_id', 'item_id'], how='left')\n",
    "    return get_test_data.cache.copy()\n",
    "get_test_data.cache = None\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use XGBoost for our model.\n",
    "# Functions here prepare data specifically in the XGBoost format.\n",
    "# make_predictions() does all the job from scratch: training, validation, predicting.\n",
    "\n",
    "# Here are all the features we use. Not many at all.\n",
    "# We use one-hot encoding for categorical features with low cardinality.\n",
    "# For high-cardinality features ('shop_id', 'item_id', 'item_category_id'),\n",
    "# simple label encoding turned out to give even better score, and certainly works faster,\n",
    "# using a lot less memory.\n",
    "# Label encoding yields pretty satisfying score, so we won't use mean encodings.\n",
    "one_hot_features = ['month_of_year', 'city_code']\n",
    "normal_features = ['date_block_num', 'item_cnt_month_lag_1', 'shop_id', 'item_id', 'item_category_id']\n",
    "all_features = np.concatenate((one_hot_features, normal_features))\n",
    "\n",
    "# Makes a matrix used for training.\n",
    "# One can provide a list of date_block_num-s for use in validation, so they will be excluded from this train set.\n",
    "# Also returns ColumnTransformer for later use in validation and test.\n",
    "def make_train_matrix(validation_months = []) ->(xgb.DMatrix, skc.ColumnTransformer):\n",
    "    sales_train = get_train_data()\n",
    "    print('making xgb train matrix...')\n",
    "    if len(validation_months) > 0:\n",
    "        print(\"Excluding validation months from train\")\n",
    "        sales_train = sales_train[~(sales_train.date_block_num.isin(validation_months))]\n",
    "    print(\"Train rows: \", sales_train.shape[0])\n",
    "    labels = sales_train['item_cnt_month'].tolist()\n",
    "    print('Used features: ' + str(all_features))\n",
    "    sales_train = sales_train[all_features]\n",
    "\n",
    "    # using n_jobs more than 1 here may cause an internal sklearn error due to large object size\n",
    "    column_transformer = skc.make_column_transformer((skp.OneHotEncoder(categories='auto'),\\\n",
    "                                                      one_hot_features),\\\n",
    "                                                     n_jobs=1, remainder='passthrough')\n",
    "    # one-hot encoding yields a sparse matrix\n",
    "    sales_train_sparse = column_transformer.fit_transform(sales_train)\n",
    "    del sales_train# to save memory\n",
    "    feature_names = column_transformer.named_transformers_['onehotencoder'].get_feature_names(one_hot_features)\n",
    "    feature_names = np.concatenate((feature_names, normal_features))\n",
    "    return (xgb.DMatrix(sales_train_sparse, label=labels, feature_names=feature_names, nthread=4), column_transformer)\n",
    "\n",
    "# Makes a validation matrix for a single month.\n",
    "def make_eval_matrix(fit_column_transformer, date_block_num = 33) -> xgb.DMatrix:\n",
    "    sales_eval = get_train_data()\n",
    "    sales_eval = sales_eval[sales_eval.date_block_num == date_block_num]\n",
    "    print(\"Adding eval matrix with row count: \", sales_eval.shape[0])\n",
    "    labels = sales_eval['item_cnt_month'].tolist()\n",
    "    sales_eval = sales_eval[all_features]\n",
    "    sales_eval_sparse = fit_column_transformer.transform(sales_eval)\n",
    "    del sales_eval\n",
    "    feature_names = fit_column_transformer.named_transformers_['onehotencoder'].get_feature_names(one_hot_features)\n",
    "    feature_names = np.concatenate((feature_names, normal_features))\n",
    "    return xgb.DMatrix(sales_eval_sparse, label=labels, feature_names=feature_names, nthread=4)\n",
    "\n",
    "# Returns trained booster (model).\n",
    "def train_model(param, num_boost_round, early_stopping_rounds, dtrain, deval = []) -> xgb.Booster:\n",
    "    watchlist = [(dtrain,'train')]\n",
    "    evalnum = 1;\n",
    "    for eval_matrix in deval:\n",
    "        watchlist.append((eval_matrix,'eval' + str(evalnum)))\n",
    "        evalnum += 1\n",
    "    evals_result = dict()\n",
    "    return xgb.train(param, dtrain, num_boost_round, evals = watchlist, evals_result= evals_result,\\\n",
    "                     verbose_eval = True, early_stopping_rounds = early_stopping_rounds)\n",
    "\n",
    "# Makes a test matrix ready for making predictions.\n",
    "def make_dtest(fit_column_transformer) -> xgb.DMatrix:\n",
    "    test_set = get_test_data()[all_features]\n",
    "    ts_sparse = fit_column_transformer.transform(test_set)\n",
    "    feature_names = fit_column_transformer.named_transformers_['onehotencoder'].get_feature_names(one_hot_features)\n",
    "    feature_names = np.concatenate((feature_names, normal_features))\n",
    "    return xgb.DMatrix(ts_sparse, feature_names=feature_names, nthread=4)\n",
    "\n",
    "# Returns a trained model and a DataFrame with predictions.\n",
    "# Prepares all data internally, we only specify parameters for training and validation.\n",
    "def make_predictions(param, num_boost_round, early_stopping_rounds, validation_months = [])\\\n",
    "-> (xgb.Booster, pd.DataFrame):\n",
    "    dtrain, column_transformer = make_train_matrix(validation_months)\n",
    "    deval = []\n",
    "    for validation_month in validation_months:\n",
    "        deval.append(make_eval_matrix(column_transformer, validation_month))\n",
    "    booster = train_model(param, num_boost_round, early_stopping_rounds, dtrain, deval)\n",
    "    del dtrain\n",
    "    del deval\n",
    "    dtest = make_dtest(column_transformer)\n",
    "    predictions = booster.predict(dtest)\n",
    "    test_set = get_test_data()\n",
    "    test_set['item_cnt_month'] = predictions\n",
    "    test_set.item_cnt_month = test_set.item_cnt_month.clip(0, 20)\n",
    "    return booster, test_set\n",
    "\n",
    "# Makes predictions using pre-trained model\n",
    "def predict_from_saved_model() -> (xgb.Booster, pd.DataFrame):\n",
    "    column_transformer = make_train_matrix([])[1]\n",
    "    booster = xgb.Booster({'nthread': 4})\n",
    "    booster.load_model(pre_trained_model)\n",
    "    dtest = make_dtest(column_transformer)\n",
    "    predictions = booster.predict(dtest)\n",
    "    test_set = get_test_data()\n",
    "    test_set['item_cnt_month'] = predictions\n",
    "    test_set.item_cnt_month = test_set.item_cnt_month.clip(0, 20)\n",
    "    return booster, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making xgb train matrix...\n",
      "Train rows:  7068600\n",
      "Used features: ['month_of_year' 'city_code' 'date_block_num' 'item_cnt_month_lag_1'\n",
      " 'shop_id' 'item_id' 'item_category_id']\n"
     ]
    }
   ],
   "source": [
    "# Now it is time to run it.\n",
    "# By default, a pre-trained model is used. If you want, set \"use_pre_trained_model\" to False.\n",
    "use_pre_trained_model = True\n",
    "if use_pre_trained_model:\n",
    "    booster, preds = predict_from_saved_model()\n",
    "else:\n",
    "    # max_depth, eta and num_boost_round are set to suboptimal values after some tuning.\n",
    "    # This model was already validated and now we aim to make best possible predictions for test.\n",
    "    # So, we use all given data for training and do no validation.\n",
    "    # num_boost_round = 20 is also tuned, so no need for early stopping here.\n",
    "    param = {'max_depth':8, 'eta':0.2, 'verbosity':1, 'objective':'reg:linear', 'eval_metric':'rmse'}\n",
    "    booster, preds = make_predictions(param, num_boost_round = 20, early_stopping_rounds = None, validation_months = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model gives ~1.01 LB score.<br>\n",
    "We are going to improve it to ~0.96 using manually examined test data.<br>\n",
    "Many items in the test set (~360) have no sales in train.<br>\n",
    "By manually examining these items by their name and searching them on the internet,<br>\n",
    "one can find a lot of new releases (that appeared around November 2015).<br>\n",
    "(Though, it may be harder if you don't know Russian).<br>\n",
    "Our model poorly handles such items, so we will manually fix predictions for them.<br>\n",
    "I actually examined all the new items and chose a subset of them, that gives most potential for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The item_id-s here are manually set after analysis of the new items.\n",
    "\n",
    "# These items were definitely on the market before November 2015.\n",
    "# We can infer that nobody buys them.\n",
    "def is_definitely_old_release(item) -> bool:\n",
    "    return item.item_id in\\\n",
    "           [6439,15183,639,640,762,22137,18627,14739,10054,10310,8850,8957,83,12441,19651,22035,12574,9030,1193]\n",
    "\n",
    "# New albums issued by Bryan Adams, Rod Steward and other very popular bands.\n",
    "def new_hot_musical_release(item) -> bool:\n",
    "    return item.item_id in [8890, 5320, 3604, 3984, 6335, 2569, 1732, 1246, 1252, 3271, 15553, 5025,\\\n",
    "                            7669, 19219, 3908, 4642, 5064, 1683, 12920]\n",
    "\n",
    "# These give the most effect.\n",
    "def new_hot_computer_game(item) -> bool:\n",
    "    # 1. Fallout 4\n",
    "    # 2. Football Manager 2016\n",
    "    # 3. Rise of the Tomb Raider\n",
    "    # 4. Call of Duty: Black Ops III\n",
    "    # 5. Star Wars: Battlefront\n",
    "    # 6. Starcraft II: Legacy Of The Void\n",
    "    # 7. Assassin's Creed new release\n",
    "    # 8. Need for Speed - 2015\n",
    "    return item.item_id in [3407, 3408, 3405]\\\n",
    "        or item.item_id in [3538]\\\n",
    "        or item.item_id in [6152, 6153]\\\n",
    "        or item.item_id in [2327, 2322, 2323, 2326, 2328, 2325]\\\n",
    "        or item.item_id in [6729, 6731, 6732, 6730, 6733, 6734]\\\n",
    "        or item.item_id in [6742, 6743]\\\n",
    "        or item.item_id in [1580, 1585, 1577, 1574, 1575]\\\n",
    "        or item.item_id in [5268]\n",
    "\n",
    "# This seems to be not so popular as games above.\n",
    "def new_normal_plus_computer_game(item) -> bool:\n",
    "    # Anno 2205\n",
    "    return item.item_id in [1437]\n",
    "\n",
    "# Even less popular but of some interest.\n",
    "def new_normal_computer_game(item) -> bool:\n",
    "    # Wasteland 2: Director's Cut \n",
    "    # Crew. Wild Run Edition\n",
    "    # Divinity. Original Sin: Enhanced Edition\n",
    "    return item.item_id in [7782]\\\n",
    "        or item.item_id in [2427]\\\n",
    "        or item.item_id in [2966]\n",
    "\n",
    "# Items that are exclusive to certain region.\n",
    "def new_local_release(item) -> bool:\n",
    "    return item.item_id in [21467,19155,8993,10483,10372]\n",
    "\n",
    "# These are very popular.\n",
    "def new_hot_movie(item) -> bool:\n",
    "    return item.item_id in [14647,14648]\n",
    "\n",
    "# Less popular but of some interest.\n",
    "def new_normal_plus_movie(item) -> bool:\n",
    "    return item.item_id in [13804,13805,18174]\n",
    "\n",
    "# So, what are we going to do with all these items?\n",
    "# Consider Fallout 4 as an example.\n",
    "# It is reasonable to expect such a game to have nearly the best sales among all PC games in the current shop.\n",
    "# Also, recall that our predictions are clipped to 20 - it greatly helps not to make a big miss here.\n",
    "# So, we are going to count maximum sales for the previous month among each shop for current item's category.\n",
    "# And we then use it as a baseline.\n",
    "# For hot computer games, for example, we can just set our predictions to the max.\n",
    "# For less popular items we may multipy max by some number, e.g. 0.5.\n",
    "def get_max_shop_category_sales(predictions) -> pd.DataFrame:\n",
    "    print('making max shop-category sales...')\n",
    "    last_month_sales = get_train_data().query('date_block_num == 33')[['shop_id', 'item_category_id', 'item_cnt_month']]\n",
    "    \n",
    "    max_sales = last_month_sales.groupby(['shop_id', 'item_category_id'], as_index=False)\\\n",
    "                                .agg({'item_cnt_month': np.max})\\\n",
    "                                .rename(columns={'item_cnt_month': 'max_sales'})\n",
    "    \n",
    "    # just to make this function more generic - item_category_id may or may not be in the passed dataframe\n",
    "    if 'item_category_id' in predictions.columns:\n",
    "        return predictions.merge(max_sales, how='left', on=['shop_id', 'item_category_id'])\n",
    "    else:\n",
    "        items = pd.read_csv(comp_data + 'items.csv')[['item_id', 'item_category_id']]\n",
    "        result = predictions.merge(items, how='left', on = ['item_id'])\n",
    "        return result.merge(max_sales, how='left', on=['shop_id', 'item_category_id'])\n",
    "\n",
    "# Finds new items in the dataframe and adjusts predictions for them.\n",
    "def fix_unknown_samples(predictions) -> pd.DataFrame:\n",
    "    result = get_max_shop_category_sales(predictions.copy())    \n",
    "    \n",
    "    print('adjusting predictions...')\n",
    "    # Here we adjust the predictions.\n",
    "    # Suboptimal coefficients were found with a little bit of LB probing.\n",
    "    result.loc[(result.apply(new_hot_computer_game, axis=1)),'item_cnt_month'] = result.max_sales\n",
    "    result.loc[(result.apply(new_normal_computer_game, axis=1)),'item_cnt_month'] = result.max_sales * 0.12\n",
    "    result.loc[(result.apply(new_normal_plus_computer_game, axis=1)),'item_cnt_month'] = result.max_sales * 0.3\n",
    "    result.loc[(result.apply(new_hot_movie, axis=1)),'item_cnt_month'] = result.max_sales * 0.8\n",
    "    result.loc[(result.apply(new_normal_plus_movie, axis=1)),'item_cnt_month'] = result.max_sales * 0.3\n",
    "    result.loc[(result.apply(new_hot_musical_release, axis=1)),'item_cnt_month'] = result.max_sales * 0.22\n",
    "\n",
    "    # Since these items are local to some region and we don't know the region exactly,\n",
    "    # it seems better to just set them to 0.\n",
    "    result.loc[(result.apply(new_local_release, axis=1)),'item_cnt_month'] = 0\n",
    "    result.loc[(result.apply(is_definitely_old_release, axis=1)),'item_cnt_month'] = 0\n",
    "    \n",
    "    result = result.drop(['max_sales'], axis=1)\n",
    "    result.item_cnt_month = result.item_cnt_month.clip(0, 20)# just for clarity\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making max shop-category sales...\n",
      "adjusting predictions...\n"
     ]
    }
   ],
   "source": [
    "preds_fixed = fix_unknown_samples(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's it\n",
    "preds_fixed[['ID', 'item_cnt_month']].to_csv(submissions + 'predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
